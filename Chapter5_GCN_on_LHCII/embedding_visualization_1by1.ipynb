{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Could not find module 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\torch_sparse\\_version_cpu.pyd' (or one of its dependencies). Try using the full path with constructor syntax.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c58fcefc159c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGCNConv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch_geometric\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mis_debug_enabled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset_debug\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch_geometric\\data\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtemporal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTemporalData\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0min_memory_dataset\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInMemoryDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch_geometric\\data\\data.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch_sparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcoalesce\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSparseTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m from torch_geometric.utils import (contains_isolated_nodes,\n\u001b[0;32m     10\u001b[0m                                    contains_self_loops, is_undirected)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch_sparse\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;34m'_saint'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_sample'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_ego_sample'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_relabel'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m ]:\n\u001b[1;32m---> 14\u001b[1;33m     torch.ops.load_library(importlib.machinery.PathFinder().find_spec(\n\u001b[0m\u001b[0;32m     15\u001b[0m         f'{library}_{suffix}', [osp.dirname(__file__)]).origin)\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\_ops.py\u001b[0m in \u001b[0;36mload_library\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[1;31m# static (global) initialization code in order to register custom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[1;31m# operators with the JIT.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloaded_libraries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\ctypes\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Could not find module 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\torch_sparse\\_version_cpu.pyd' (or one of its dependencies). Try using the full path with constructor syntax."
     ]
    }
   ],
   "source": [
    "#this script is to use tranied model and some data points to\n",
    "#visualize the last \"fully-connected layer\" of GCN.\n",
    "\n",
    "import torch\n",
    "#from GCN_regression import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from sklearn.preprocessing import *\n",
    "\n",
    "from my_dataset_1by1 import *\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_regression_Net_single(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    feed the data one-by-one version.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, nhid1, nhid2, nhid3):\n",
    "        super(GCN_regression_Net_single, self).__init__()\n",
    "        self.conv1 = GCNConv(n_features, nhid1)\n",
    "        self.conv2 = GCNConv(nhid1, nhid2)\n",
    "        self.conv3 = GCNConv(nhid2, nhid3)\n",
    "        #self.bn = torch.nn.BatchNorm1d(n_nodes)\n",
    "        #self.dropout = dropout\n",
    "        #self.linear1 = torch.nn.Linear(nhid3, 1)    #used in previous patch, reduce nhid first\n",
    "        self.linear1 = torch.nn.Linear(nhid3, 1)\n",
    "        self.linear2 = torch.nn.Linear(508, 1)\n",
    "        #self.batch_size = batch_size\n",
    "        #self.maxpool = torch.nn.MaxPool2d(4, stride = 4) # max pooling with square window of size=3, stride=2\n",
    "        #no maxpooling because it will reduce the feature channel.\n",
    "        \n",
    "    def forward(self, X, edge_index, edge_weight):\n",
    "        x, edge_index, edge_weight = X, edge_index, edge_weight #X has shape[batch_size * n_node]\n",
    "\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv3(x, edge_index, edge_weight)    # X in shape: [n_nodes * batch_size, nhid3]\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = F.dropout(x, training=self.training) \n",
    "        \n",
    "        x = self.linear1(x) #output shape in [508,1]  \n",
    "        \n",
    "        #print(\"the output layer shape is:\")\n",
    "        #print(x.shape)\n",
    "        \n",
    "        \n",
    "        x = torch.squeeze(x)    #output shape in [508]\n",
    "        #print(type(x))\n",
    "        x = self.linear2(x) #output shape in [1]\n",
    "        x = torch.squeeze(x)\n",
    "        #print(x.shape)\n",
    "        #x = torch.reshape(x, (self.batch_size, -1)) # output shape in [batch_size, n_node]\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.random.manual_seed(1)\n",
    "device = torch.device('cpu')\n",
    "\n",
    "dataset_size = 3000\n",
    "perm = torch.randperm(dataset_size).numpy()\n",
    "partition = {}\n",
    "partition[\"train\"] = perm[:int(dataset_size*8/10)]\n",
    "partition[\"validation\"] = perm[int(dataset_size*8/10):int(dataset_size*9/10)]\n",
    "partition[\"test\"] = perm[int(dataset_size*9/10):]\n",
    "\n",
    "model = GCN_regression_Net_single(n_features = 6, \n",
    "                               nhid1 = 32, \n",
    "                               nhid2 = 64, \n",
    "                               nhid3 = 128).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1E-6, weight_decay=5e-4)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "earlystopping = True\n",
    "best_epoch_num = 0\n",
    "best_loss_valid = 1e10\n",
    "patient = 100\n",
    "prefix = \"gcn_net_trained_18_single_MSELoss\"\n",
    "max_epochs = 500\n",
    "    \n",
    "train_set = torch.utils.data.Subset(My_dataset(), partition[\"train\"])\n",
    "validation_set = torch.utils.data.Subset(My_dataset(), partition[\"validation\"])\n",
    "test_set = torch.utils.data.Subset(My_dataset(), partition[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.linear2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#here we load the data and trained GCN model.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    hook_outputs = []\n",
    "    total_outputs = []\n",
    "    total_labels = []\n",
    "    \n",
    "    PATH = './models/gcn_net_trained_18_single_MSELoss.pt.final'\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "       \n",
    "    \n",
    "    #define a hook to strip embedding in each batch.\n",
    "    def hook(module, input, output):\n",
    "        print(module)\n",
    "        hook_outputs.append(input)\n",
    "        print(\"printing hook output\")\n",
    "        print(hook_outputs[0][0])\n",
    "        print(hook_outputs[0][0].shape)\n",
    "        #print(type(hook_outputs))\n",
    "    \n",
    "    #going through the selected dataset, without the gradient.(no change on model)\n",
    "    h = model.linear2.register_forward_hook(hook)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, datapoint in enumerate(test_set):\n",
    "            X, edge_index, edge_weight, y = datapoint.X, datapoint.edge_index, datapoint.edge_attr, datapoint.y\n",
    "            \n",
    "            #transfer to GPU\n",
    "            #x, edge_index, edge_attr, y = Variable(torch.as_tensor(x)).to(device), Variable(torch.as_tensor(edge_index)).to(device), Variable(torch.as_tensor(edge_attr)).to(device), Variable(torch.as_tensor(y)).to(device)\n",
    "            \n",
    "            X, edge_index, edge_weight = Variable(torch.as_tensor(X, dtype=torch.float32)).to(device), edge_index.to(device), edge_weight.to(device)\n",
    "            #y = y.to(device)\n",
    "            y = Variable(torch.as_tensor(y, dtype=torch.float32)).to(device)\n",
    "            \n",
    "            #optimizer.zero_grad()\n",
    "            \n",
    "            prediction_train = model(X, edge_index, edge_weight) #forward \n",
    "            loss = loss_func(prediction_train, y)\n",
    "        \n",
    "            #loss.backward() # backward\n",
    "            #optimizer.step() # optimizer step\n",
    "            #optimizer.zero_grad()\n",
    "            if i == 0:\n",
    "                print(\"hooking datapoint no. %d\" %i)\n",
    "                #print(np.expand_dims(hook_outputs[0][0].detach().numpy(), axis = 1).shape)\n",
    "                total_outputs = np.expand_dims(hook_outputs[0][0].detach().numpy(), axis = 1)\n",
    "                total_labels.append(y.item())\n",
    "                #print(total_labels.shape)\n",
    "                #print(max(batch_outputs[0][0].detach().numpy()))\n",
    "            else:\n",
    "                print(\"hooking datapoint no. %d\" %i)\n",
    "                total_outputs = np.concatenate((total_outputs, np.expand_dims(hook_outputs[0][0].detach().numpy(), axis = 1)), axis = 1)\n",
    "                print(total_outputs)\n",
    "                #print(total_outputs.shape)\n",
    "                total_labels.append(y.item())\n",
    "                #print(total_labels.shape)\n",
    "                #print(max(batch_outputs[0][0].detach().numpy()))\n",
    "            #total_outputs.append(batch_outputs[0][0].detach().numpy()) #record the embedding for this batch.\n",
    "            if i == 10: break\n",
    "    h.remove()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./hooked_output.npy\", total_outputs.T)\n",
    "np.save(\"./hooked_label_ref.npy\", total_labels)\n",
    "hooked_output = total_outputs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=10, svd_solver='full')\n",
    "print(total_outputs)\n",
    "print(len(total_labels))\n",
    "pca.fit(hooked_output)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_result = pca.fit_transform(hooked_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pca_result[:,0]))\n",
    "print(pca_result[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lifetime_mapping(labels):\n",
    "    #take in labels, numbers, output the index\n",
    "    output = []\n",
    "    for element in labels:\n",
    "        if element < 300:\n",
    "            output.append(int(0))\n",
    "        elif element >=300 and element <600:\n",
    "            output.append(int(1))\n",
    "        elif element >=600:\n",
    "            output.append(int(2))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diction={'x(PC 1)': pca_result[:,0],'y(PC 2)': pca_result[:,1], 'z(PC 3)': pca_result[:,2], 'lifetime': lifetime_mapping(total_labels)}\n",
    "df = pd.DataFrame(data=diction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.scatterplot(x=\"x(PC 1)\", y=\"y(PC 2)\", hue = 'lifetime', size = 'lifetime', data=df, alpha= 0.8)\n",
    "#plt.scatter(x=pca_result[:,0], y=pca_result[:,1],alpha= 0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection = '3d')\n",
    "\n",
    "x = df['x(PC 1)']\n",
    "y = df['y(PC 2)']\n",
    "z = df['z(PC 3)']\n",
    "\n",
    "ax.set_xlabel(\"PC 1\")\n",
    "ax.set_ylabel(\"PC 2\")\n",
    "ax.set_zlabel(\"PC 3\")\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap = ListedColormap(sns.color_palette(\"husl\", 256).as_hex())\n",
    "\n",
    "ax.scatter(x, y, z, c=df['lifetime'], cmap=cmap)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for name, param in model.named_parameters():\n",
    "    #if name == 'linear1.weight':\n",
    "    if name == 'conv3.weight':\n",
    "        #print(param.detach().numpy())\n",
    "        weights = param.detach().numpy()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#T-SNE visualization.\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_outputs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_results = tsne.fit_transform(total_outputs)\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diction={'x': tsne_results[:,0],'y': tsne_results[:,1], 'lifetime': lifetime_mapping(total_labels)}\n",
    "df2 = pd.DataFrame(data=diction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x='x', y='y',\n",
    "    #hue=\"lifetime\",\n",
    "    #palette=sns.color_palette(\"hls\", 10),\n",
    "    data=df2,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
